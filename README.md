# Meta-Evaluation of Reference-Free LLM Metrics for RAG

This repository contains the code and data used in the paper:

**Meta-Evaluation of LLM-Based Reference-Free Metrics for RAG**  
Xinyuan Cheng Â· LMU Munich

## ğŸ§  Overview

We evaluate the robustness and validity of various **reference-free evaluation metrics** for Retrieval-Augmented Generation (RAG), including:

- **Direct Prompting** (DP-Free & DP-Token)
- **G-Eval**
- **GPTScore**
- **RAGAS**

Our evaluation is conducted on the [WikiEval benchmark](https://huggingface.co/datasets/ExplodingGradients/WikiEval) and covers three quality dimensions:

- **Faithfulness**
- **Answer Relevance**
- **Context Relevance**

We assess both **pairwise accuracy** and **correlation with human judgments**, and analyze the effects of prompt format and LLM confidence.

## ğŸ“ Project Structure

<pre> ```text project-name/ â”œâ”€â”€ folder1/ â”‚ â”œâ”€â”€ file1.py â”‚ â””â”€â”€ file2.py â”œâ”€â”€ folder2/ â”‚ â””â”€â”€ subfolder/ â”‚ â””â”€â”€ file3.py â””â”€â”€ README.md ``` </pre>
