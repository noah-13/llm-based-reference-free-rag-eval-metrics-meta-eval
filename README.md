# Meta-Evaluation of Reference-Free LLM Metrics for RAG

This repository contains the code and data used in the paper:

**Meta-Evaluation of LLM-Based Reference-Free Metrics for RAG**  
Xinyuan Cheng Â· LMU Munich

## ğŸ§  Overview

We evaluate the robustness and validity of various **reference-free evaluation metrics** for Retrieval-Augmented Generation (RAG), including:

- **Direct Prompting** (DP-Free & DP-Token)
- **G-Eval**
- **GPTScore**
- **RAGAS**

Our evaluation is conducted on the [WikiEval benchmark](https://huggingface.co/datasets/ExplodingGradients/WikiEval) and covers three quality dimensions:

- **Faithfulness**
- **Answer Relevance**
- **Context Relevance**

We assess both **pairwise accuracy** and **correlation with human judgments**, and analyze the effects of prompt format and LLM confidence.

## ğŸ“ Project Structure

<pre>llm-metric-metaeval/
â”œâ”€â”€ experiment/
â”‚ â”œâ”€â”€ scripts/ # Scripts for running metric evaluations
â”‚ â”œâ”€â”€ input/ # Input files: QCA data
â”‚ â”œâ”€â”€ output/ # Metric outputs (CSV)
â”œâ”€â”€ results_analysis/
â”‚ â”œâ”€â”€ scripts/ # Scripts for accuracy, correlation, and robustness analysis
â”‚ â”œâ”€â”€ output/ # Analysis Results (CSV)</pre>

## ğŸš€ Run Experiments
Navigate to experiment/scripts/ and run any of the metric scripts. Example:

<pre> python ragas_ff.py --input_csv ../input/ff.csv --output_csv ../output/ragas_ff_output.csv </pre>

## ğŸ“Œ Naming Conventions
The files in follow the naming pattern:

ff â€“ Faithfulness

ar â€“ Answer Relevance

cr â€“ Context Relevance

prompt - DP-free + DP-token + G-eval

variants - including variants of prompt

For example:

ragas_cr.py evaluates context relevance using RAGAS

prompt_ar_variants.py evaluates answer relevance using DP-free, DP-token and G-eval, including prompt variants
